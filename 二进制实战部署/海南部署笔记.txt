root@node-1:/# cat /etc/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \
  --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \
  --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \
  --kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \
  --leader-elect=true \
  --bind-address=0.0.0.0 \
  --port=0 \
  --v=1
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target


root@node-2:/etc/kubernetes/manifests#  vi /etc/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \
  --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \
  --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \
  --kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \
  --leader-elect=true \
  --bind-address=0.0.0.0 \
  --port=0 \
  --v=1
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target

# 集群基础POD
root@node-1:~/deploy-work/ingress# kubectl get pods -A
NAMESPACE       NAME                                        READY   STATUS    RESTARTS   AGE
ingress-nginx   nginx-ingress-controller-84964dddbc-plmhv   1/1     Running   0          47s
kube-system     calico-kube-controllers-858c9597c8-lqg4g    1/1     Running   0          2d16h
kube-system     calico-node-4xf78                           1/1     Running   0          2d16h
kube-system     calico-node-ccdq9                           1/1     Running   0          2d16h
kube-system     calico-node-dszd2                           1/1     Running   0          2d16h
kube-system     calico-node-g2wbg                           1/1     Running   0          2d16h
kube-system     calico-node-m5h7l                           1/1     Running   0          2d16h
kube-system     calico-node-mdmp5                           1/1     Running   0          2d16h
kube-system     calico-node-v62js                           1/1     Running   0          2d16h
kube-system     coredns-587b9b686d-268wh                    1/1     Running   0          15m
kube-system     coredns-587b9b686d-cw5kk                    1/1     Running   0          14m
kube-system     nginx-proxy-node-3                          1/1     Running   0          2d18h
kube-system     nginx-proxy-node-4                          1/1     Running   0          2d19h
kube-system     nginx-proxy-node-5                          1/1     Running   0          2d19h
kube-system     nginx-proxy-node-6                          1/1     Running   0          2d20h
kube-system     nginx-proxy-node-7                          1/1     Running   0          2d19h
kube-system     nginx-proxy-node-8                          1/1     Running   0          2d19h
kube-system     nodelocaldns-22dsj                          1/1     Running   0          55m
kube-system     nodelocaldns-4wvhf                          1/1     Running   0          55m
kube-system     nodelocaldns-gkrnm                          1/1     Running   0          55m
kube-system     nodelocaldns-lrmsz                          1/1     Running   0          55m
kube-system     nodelocaldns-mpb4s                          1/1     Running   0          55m
kube-system     nodelocaldns-n44t6                          1/1     Running   0          55m
kube-system     nodelocaldns-phbts                          1/1     Running   0          55m
root@node-1:~/deploy-work/ingress# clear
root@node-1:~/deploy-work/ingress# kubectl get pods -A
NAMESPACE       NAME                                        READY   STATUS    RESTARTS   AGE
ingress-nginx   nginx-ingress-controller-84964dddbc-plmhv   1/1     Running   0          52s
kube-system     calico-kube-controllers-858c9597c8-lqg4g    1/1     Running   0          2d16h
kube-system     calico-node-4xf78                           1/1     Running   0          2d16h
kube-system     calico-node-ccdq9                           1/1     Running   0          2d16h
kube-system     calico-node-dszd2                           1/1     Running   0          2d16h
kube-system     calico-node-g2wbg                           1/1     Running   0          2d16h
kube-system     calico-node-m5h7l                           1/1     Running   0          2d16h
kube-system     calico-node-mdmp5                           1/1     Running   0          2d16h
kube-system     calico-node-v62js                           1/1     Running   0          2d16h
kube-system     coredns-587b9b686d-268wh                    1/1     Running   0          15m
kube-system     coredns-587b9b686d-cw5kk                    1/1     Running   0          15m
kube-system     nginx-proxy-node-3                          1/1     Running   0          2d18h
kube-system     nginx-proxy-node-4                          1/1     Running   0          2d19h
kube-system     nginx-proxy-node-5                          1/1     Running   0          2d19h
kube-system     nginx-proxy-node-6                          1/1     Running   0          2d20h
kube-system     nginx-proxy-node-7                          1/1     Running   0          2d19h
kube-system     nginx-proxy-node-8                          1/1     Running   0          2d19h
kube-system     nodelocaldns-22dsj                          1/1     Running   0          56m
kube-system     nodelocaldns-4wvhf                          1/1     Running   0          56m
kube-system     nodelocaldns-gkrnm                          1/1     Running   0          56m
kube-system     nodelocaldns-lrmsz                          1/1     Running   0          56m
kube-system     nodelocaldns-mpb4s                          1/1     Running   0          56m
kube-system     nodelocaldns-n44t6                          1/1     Running   0          56m
kube-system     nodelocaldns-phbts                          1/1     Running   0          56m

# 集群基础镜像
root@node-2:~# crictl images
IMAGE                                                          TAG                 IMAGE ID            SIZE
docker.io/calico/cni                                           v3.22.1             2a8ef6985a3e5       80.5MB
docker.io/calico/node                                          v3.22.1             7a71aca7b60fc       69.6MB
docker.io/calico/pod2daemon-flexvol                            v3.22.1             17300d20daf93       8.46MB
docker.io/library/nginx                                        1.19                f0b8a9a541369       53.7MB
k8s.gcr.io/pause                                               3.2                 80d28bedfe5de       298kB
registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/pause   3.2                 80d28bedfe5de       298kB


docker pull docker.io/calico/cni:v3.22.1           
docker pull docker.io/calico/node:v3.22.1 
docker pull docker.io/calico/pod2daemon-flexvol:v3.22.1
docker pull docker.io/coredns/coredns:1.6.7
docker pull docker.io/library/mysql:5.7
docker pull docker.io/library/nginx:1.19 
docker pull k8s.gcr.io/pause:3.2
docker pull registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/pause:3.2                 






# Master进程
root@node-2:~# netstat -lptn
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      56854/kubelet       
tcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      56836/kube-proxy    
tcp        0      0 210.37.68.74:10250      0.0.0.0:*               LISTEN      56854/kubelet       
tcp        0      0 127.0.0.1:9099          0.0.0.0:*               LISTEN      121124/calico-node  
tcp        0      0 0.0.0.0:179             0.0.0.0:*               LISTEN      121347/bird         
tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      668/systemd-resolve 
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      842/sshd            
tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      214681/cupsd        
tcp        0      0 127.0.0.1:44637         0.0.0.0:*               LISTEN      8699/containerd     
tcp6       0      0 :::10251                :::*                    LISTEN      71970/kube-schedule 
tcp6       0      0 :::6443                 :::*                    LISTEN      148693/kube-apiserv 
tcp6       0      0 :::10252                :::*                    LISTEN      148700/kube-control 
tcp6       0      0 :::10256                :::*                    LISTEN      56836/kube-proxy    
tcp6       0      0 :::10257                :::*                    LISTEN      148700/kube-control 
tcp6       0      0 :::10259                :::*                    LISTEN      71970/kube-schedule 
tcp6       0      0 :::22                   :::*                    LISTEN      842/sshd            
tcp6       0      0 ::1:631                 :::*                    LISTEN      214681/cupsd 

# Worker进程
root@node-3:~# netstat  -lptn
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      120747/kubelet      
tcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      120725/kube-proxy   
tcp        0      0 127.0.0.1:33257         0.0.0.0:*               LISTEN      70227/containerd    
tcp        0      0 210.37.68.75:10250      0.0.0.0:*               LISTEN      120747/kubelet      
tcp        0      0 127.0.0.1:9099          0.0.0.0:*               LISTEN      188138/calico-node  
tcp        0      0 127.0.0.1:6443          0.0.0.0:*               LISTEN      117542/nginx: maste 
tcp        0      0 0.0.0.0:8081            0.0.0.0:*               LISTEN      117542/nginx: maste 
tcp        0      0 0.0.0.0:179             0.0.0.0:*               LISTEN      188331/bird         
tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      665/systemd-resolve 
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      813/sshd            
tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      59047/cupsd         
tcp6       0      0 :::10256                :::*                    LISTEN      120725/kube-proxy   
tcp6       0      0 :::22                   :::*                    LISTEN      813/sshd            
tcp6       0      0 ::1:631                 :::*                    LISTEN      59047/cupsd 

# 解决coredns OOMKilled
kubectl edit deployment coredns -n kube-system

>>>
root@node-1:~/deploy-work/coredns# kubectl edit deployment coredns -n kube-system
# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"addonmanager.kubernetes.io/mode":"Reconcile","k8s-app":"kube-dns","kubernetes.io/name":"coredns"},"name":"coredns","namespace":"kube-system"},"spec":{"replicas":2,"selector":{"matchLabels":{"k8s-app":"kube-dns"}},"strategy":{"rollingUpdate":{"maxSurge":"10%","maxUnavailable":0},"type":"RollingUpdate"},"template":{"metadata":{"annotations":{"seccomp.security.alpha.kubernetes.io/pod":"runtime/default"},"labels":{"k8s-app":"kube-dns"}},"spec":{"affinity":{"nodeAffinity":{"preferredDuringSchedulingIgnoredDuringExecution":[{"preference":{"matchExpressions":[{"key":"node-role.kubernetes.io/master","operator":"In","values":[""]}]},"weight":100}]},"podAntiAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":[{"labelSelector":{"matchLabels":{"k8s-app":"kube-dns"}},"topologyKey":"kubernetes.io/hostname"}]}},"containers":[{"args":["-conf","/etc/coredns/Corefile"],"image":"docker.io/coredns/coredns:1.6.7","imagePullPolicy":"IfNotPresent","livenessProbe":{"failureThreshold":10,"httpGet":{"path":"/health","port":8080,"scheme":"HTTP"},"successThreshold":1,"timeoutSeconds":5},"name":"coredns","ports":[{"containerPort":53,"name":"dns","protocol":"UDP"},{"containerPort":53,"name":"dns-tcp","protocol":"TCP"},{"containerPort":9153,"name":"metrics","protocol":"TCP"}],"readinessProbe":{"failureThreshold":10,"httpGet":{"path":"/ready","port":8181,"scheme":"HTTP"},"successThreshold":1,"timeoutSeconds":5},"resources":{"limits":{"memory":"170Mi"},"requests":{"cpu":"100m","memory":"70Mi"}},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"add":["NET_BIND_SERVICE"],"drop":["all"]},"readOnlyRootFilesystem":true},"volumeMounts":[{"mountPath":"/etc/coredns","name":"config-volume"}]}],"dnsPolicy":"Default","nodeSelector":{"kubernetes.io/os":"linux"},"priorityClassName":"system-cluster-critical","serviceAccountName":"coredns","tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"}],"volumes":[{"configMap":{"items":[{"key":"Corefile","path":"Corefile"}],"name":"coredns"},"name":"config-volume"}]}}}}
  creationTimestamp: "2022-03-21T03:09:03Z"
  generation: 1
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: kube-dns
    kubernetes.io/name: coredns
  name: coredns
  namespace: kube-system
  resourceVersion: "539413"
  uid: 16c40e77-48b7-4d9c-8cad-366522156a23
spec:
  progressDeadlineSeconds: 600
  replicas: 2
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kube-dns
  strategy:
    rollingUpdate:
      maxSurge: 10%
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      annotations:
        seccomp.security.alpha.kubernetes.io/pod: runtime/default
      creationTimestamp: null
      labels:
        k8s-app: kube-dns
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: node-role.kubernetes.io/master
                operator: In
                values:
                - ""
            weight: 100
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                k8s-app: kube-dns
            topologyKey: kubernetes.io/hostname
      containers:
      - args:
        - -conf
        - /etc/coredns/Corefile
        image: docker.io/coredns/coredns:1.6.7
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 10
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        name: coredns
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        readinessProbe:
          failureThreshold: 10
          httpGet:
            path: /ready
            port: 8181
            scheme: HTTP
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          limits:
            #memory: 170Mi
			memory: 2048Mi
          requests:
            cpu: 100m
            #memory: 70Mi
			memory: 1024Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - all
          readOnlyRootFilesystem: true
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /etc/coredns
          name: config-volume
      dnsPolicy: Default
      nodeSelector:
        kubernetes.io/os: linux
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: coredns
      serviceAccountName: coredns
      terminationGracePeriodSeconds: 30
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      volumes:
      - configMap:
          defaultMode: 420
          items:
          - key: Corefile
            path: Corefile
          name: coredns
        name: config-volume
status:
  conditions:
  - lastTransitionTime: "2022-03-21T03:09:03Z"
    lastUpdateTime: "2022-03-21T03:09:03Z"
    message: Deployment does not have minimum availability.
    reason: MinimumReplicasUnavailable
    status: "False"
    type: Available
  - lastTransitionTime: "2022-03-21T03:09:03Z"
    lastUpdateTime: "2022-03-21T03:20:37Z"
    message: ReplicaSet "coredns-84646c885d" is progressing.
    reason: ReplicaSetUpdated
    status: "True"
    type: Progressing
  observedGeneration: 1
  replicas: 2
  unavailableReplicas: 2
  updatedReplicas: 2               
<<<



# 修改coredns configmap
# https://blog.51cto.com/hexiaoshuai/2812394
kubectl edit configmap coredns -n kube-system

# Dashboard URL
https://210.37.68.78:30403/
# Dashboard Token
kubectl get secret -A
kubectl -n kubernetes-dashboard get sa
kubectl describe secret/kubernetes-dashboard-token-mq5gq -n kubernetes-dashboard

eyJhbGciOiJSUzI1NiIsImtpZCI6ImhmVDB6TUNGS2MtU1dNdlhEc1dHUkhJNGU3WThKcXRfcFZxR194YUtZM28ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC10b2tlbi1tcTVncSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImU4NTk2NjczLTlkOTItNDBmZi04NTc4LTdkMDBkMmU2ODJmNCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDprdWJlcm5ldGVzLWRhc2hib2FyZCJ9.aAmGxSr1ZDLZ21LQ-WwYdzOl6eZd3kZTKPbrJgbPDGxagrNKcRq8fIHHoggKED1hBtkANIt1YZ3AmBbhOI-kgtkpJ-CbjoyH7OK0iFLs5GneHUvv_hWfR5R-7JKCxbHDsRp7jET6psNHzjzCB9YV02ILBIdXTB_eHCX61oCKfeX3g7BxspVvDm9WwnZ8OZJDlLn026G2UjOJwoWOx5ZJayqRMRxPRwzhIL0P8MJJvtP9yyT645A6g7VVwdqgV3BtQiETGXl80T_67VkMFp3xv6OuPVG3DNK-0hvpp7ZFAcJ-xBYcwnTJ85e1ynLA7PCo-ulgnLTZlGMKtiXVIActnA



# 查看证书内容
openssl x509 -text -noout -in /etc/etcd/kubernetes.pem
openssl x509 -text -noout -in ./kubernetes.pem


# 删除pvc
kubectl delete pvc opspvc  -n kube-ops

# 修改Api-Server
--feature-gates=RemoveSelfLink=false
https://blog.csdn.net/ag1942/article/details/115371793


# 强制启动yaml
kubectl apply -f ./ --force



kubectl run mysql-client --image=mysql:5.7 -i --rm --restart=Never --mysql -h mysql-0.mysql <<EOF 
CREATE DATABASE test; 
CREATE TABLE test.messages (message VARCHAR(250)); 
INSERT INTO test.messages VALUES ('hello'); 
EOF 


kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --mysql -h mysql-read -e "SELECT * FROM test.messages" 


>>集群基础镜像
docker pull docker.io/calico/cni:v3.22.1           
docker pull docker.io/calico/node:v3.22.1 
docker pull docker.io/calico/pod2daemon-flexvol:v3.22.1
docker pull docker.io/coredns/coredns:1.6.7
docker pull docker.io/library/mysql:5.7
docker pull docker.io/library/nginx:1.19 
#docker pull k8s.gcr.io/pause:3.2
docker pull registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/pause:3.2
docker pull registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/dns_k8s-dns-node-cache:1.16.0

>>>
$ crictl pull registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/pause:3.2
$ ctr -n k8s.io i tag  registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/pause:3.2 k8s.gcr.io/pause:3.2
<<<

#登陆
docker login --username=duanshouzhi516518 registry.cn-qingdao.aliyuncs.com
>>推送镜像
#1、docker.io/library/mysql:5.7
docker tag 05311a87aeb4 registry.cn-qingdao.aliyuncs.com/dszk8/mysql:5.7 && \
docker push registry.cn-qingdao.aliyuncs.com/dszk8/mysql:5.7
#2、docker.io/calico/cni:v3.22.1
docker tag 2a8ef6985a3e registry.cn-qingdao.aliyuncs.com/dszk8/calico_cni:v3.22.1 && \
docker push registry.cn-qingdao.aliyuncs.com/dszk8/calico_cni:v3.22.1
#3、docker.io/calico/pod2daemon-flexvol
docker tag 17300d20daf9 registry.cn-qingdao.aliyuncs.com/dszk8/calico_pod2daemon-flexvol:v3.22.1 && \
docker push registry.cn-qingdao.aliyuncs.com/dszk8/calico_pod2daemon-flexvol:v3.22.1
#4、docker.io/calico/node:v3.22.1
docker tag 7a71aca7b60f registry.cn-qingdao.aliyuncs.com/dszk8/calico_node:v3.22.1 && \
docker push registry.cn-qingdao.aliyuncs.com/dszk8/calico_node:v3.22.1
#5、docker.io/library/nginx:1.19
docker tag f0b8a9a54136 registry.cn-qingdao.aliyuncs.com/dszk8/nginx:1.19 && \
docker push registry.cn-qingdao.aliyuncs.com/dszk8/nginx:1.19
#6、k8s.gcr.io/pause:3.2
docker tag 80d28bedfe5d registry.cn-qingdao.aliyuncs.com/dszk8/pause:3.2 && \
docker push registry.cn-qingdao.aliyuncs.com/dszk8/pause:3.2
#7、docker.io/coredns/coredns:1.6.7
docker tag 67da37a9a360 registry.cn-qingdao.aliyuncs.com/dszk8/coredns:1.6.7 && \
docker push registry.cn-qingdao.aliyuncs.com/dszk8/coredns:1.6.7 
#8、registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/dns_k8s-dns-node-cache:1.16.0
docker tag 90f9d984ec9a registry.cn-qingdao.aliyuncs.com/dszk8/dns_k8s-dns-node-cache:1.16.0 && \
docker push registry.cn-qingdao.aliyuncs.com/dszk8/dns_k8s-dns-node-cache:1.16.0
>>删除本地镜像






