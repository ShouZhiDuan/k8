# K8集群部署文档

## 一、账户信息

- 主机密码

```tex
HY@node.com
```

- 远程连接

```tex
740478454
n5298l
```

- 集群健康图

![image-20220531152849273](C:\Users\dev\AppData\Roaming\Typora\typora-user-images\image-20220531152849273.png)

## 二、服务器

### 1、资源分配

| 序号 | 系统   | 主机名                            | IP           | 内存 | 磁盘 | 角色          | 备注         |
| ---- | ------ | --------------------------------- | ------------ | ---- | ---- | ------------- | ------------ |
| 1    | Ubuntu | node-1                            | 210.37.68.73 | 8G   | 1T   | MASTER        | 单角色       |
| 2    | Ubuntu | node-2                            | 210.37.68.74 | 8G   | 1T   | MASTER/WORKER | 多角色       |
| 3    | Ubuntu | node-3                            | 210.37.68.75 | 8G   | 1T   | WORKER        | 单角色       |
| 4    | Ubuntu | node-4                            | 210.37.68.76 | 8G   | 1T   | WORKER        | 单角色       |
| 5    | Ubuntu | node-5                            | 210.37.68.77 | 8G   | 1T   | WORKER        | 单角色       |
| 6    | Ubuntu | node-6                            | 210.37.68.78 | 8G   | 1T   | ETCD/WORKER   | 多角色       |
| 7    | Ubuntu | node-7                            | 210.37.68.79 | 8G   | 1T   | ETCD/WORKER   | 多角色       |
| 8    | Ubuntu | node-8                            | 210.37.68.80 | 8G   | 1T   | ETCD/WORKER   | 多角色       |
| 9    | Ubuntu | node-Standard-PC-i440FX-PIIX-1996 | 210.37.68.72 | 8G   | 1T   | NFS           | 网络文件系统 |
### 2、证书目录

#### 2.1 Master

------

`cd /etc/kubernetes`

```shell
root@node-1:/etc/kubernetes# ll
-rw-------   1 root root  6397 3月  17 22:40 kube-controller-manager.kubeconfig
-rw-------   1 root root  6347 3月  17 22:41 kube-scheduler.kubeconfig
drwxr-xr-x   2 root root  4096 3月  17 22:33 ssl/
```

`cd /etc/kubernetes/ssl`

```shell
root@node-1:/etc/kubernetes/ssl# ll
-rw------- 1 root root 1675 3月  17 22:33 ca-key.pem
-rw-r--r-- 1 root root 1367 3月  17 22:33 ca.pem
-rw------- 1 root root 1675 3月  17 22:33 kubernetes-key.pem
-rw-r--r-- 1 root root 1639 3月  17 22:33 kubernetes.pem
-rw------- 1 root root 1675 3月  17 22:33 proxy-client-key.pem
-rw-r--r-- 1 root root 1399 3月  17 22:33 proxy-client.pem
-rw------- 1 root root 1679 3月  17 22:33 service-account-key.pem
-rw-r--r-- 1 root root 1407 3月  17 22:33 service-account.pem
```

#### 2.2 Master|Worker

------

`cd /etc/kubernetes`

```shell
root@node-2:/etc/kubernetes# ll
-rw-------   1 root root  6365 3月  18 15:22 kubeconfig
-rw-------   1 root root  6397 3月  17 22:40 kube-controller-manager.kubeconfig
-rw-r--r--   1 root root   689 3月  18 15:24 kubelet-config.yaml
-rw-r--r--   1 root root   207 3月  18 15:31 kube-proxy-config.yaml
-rw-------   1 root root  6295 3月  18 15:30 kube-proxy.kubeconfig
-rw-------   1 root root  6347 3月  17 22:41 kube-scheduler.kubeconfig
drwxr-xr-x   2 root root  4096 3月  18 17:28 manifests/ #这个目录是空的，没有文件
drwxr-xr-x   2 root root  4096 3月  18 15:16 ssl/
```

`cd /etc/kubernetes/ssl`

```shell
root@node-2:/etc/kubernetes/ssl# ll
-rw------- 1 root root 1675 3月  18 15:21 ca-key.pem
-rw-r--r-- 1 root root 1367 3月  18 15:21 ca.pem
-rw------- 1 root root 1675 3月  17 22:34 kubernetes-key.pem
-rw-r--r-- 1 root root 1639 3月  17 22:34 kubernetes.pem
-rw------- 1 root root 1675 3月  18 15:21 node-2-key.pem
-rw-r--r-- 1 root root 1456 3月  18 15:21 node-2.pem
-rw------- 1 root root 1675 3月  17 22:34 proxy-client-key.pem
-rw-r--r-- 1 root root 1399 3月  17 22:34 proxy-client.pem
-rw------- 1 root root 1679 3月  17 22:34 service-account-key.pem
-rw-r--r-- 1 root root 1407 3月  17 22:34 service-account.pem
```

#### 2.3 Worker

------

`cd /etc/kubernetes`

```shell
root@node-3:/etc/kubernetes# ll
-rw-------   1 root root  6365 3月  18 15:22 kubeconfig
-rw-r--r--   1 root root   689 3月  18 15:24 kubelet-config.yaml
-rw-r--r--   1 root root   207 3月  18 15:31 kube-proxy-config.yaml
-rw-------   1 root root  6295 3月  18 15:30 kube-proxy.kubeconfig
#单群的worker角色这个目录会有nginx-proxy.yaml文件，具体见下方
drwxr-xr-x   2 root root  4096 3月  18 15:29 manifests/ 
drwxr-xr-x   2 root root  4096 3月  18 15:21 ssl/
```

`/etc/kubernetes/manifests`

```shell
root@node-3:/etc/kubernetes/manifests# ll
-rw-r--r-- 1 root root  960 3月  18 15:29 nginx-proxy.yaml
```

`cat ./nginx-proxy.yaml`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-proxy
  namespace: kube-system
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: kube-nginx
spec:
  hostNetwork: true
  dnsPolicy: ClusterFirstWithHostNet
  nodeSelector:
    kubernetes.io/os: linux
  priorityClassName: system-node-critical
  containers:
  - name: nginx-proxy
    image: docker.io/library/nginx:1.19
    imagePullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 25m
        memory: 32M
    securityContext:
      privileged: true
    livenessProbe:
      httpGet:
        path: /healthz
        #注意宿主机模式
        port: 8081
    readinessProbe:
      httpGet:
        path: /healthz
        #注意宿主机模式
        port: 8081
    volumeMounts:
    - mountPath: /etc/nginx
      name: etc-nginx
      readOnly: true
  volumes:
  - name: etc-nginx
    hostPath:
      #挂在上面配置的/etc/nginx/nginx.conf文件
      path: /etc/nginx
```

`cd  /etc/kubernetes/ssl`

```shell
root@node-3:/etc/kubernetes/ssl# ll
-rw------- 1 root root 1675 3月  18 15:21 ca-key.pem
-rw-r--r-- 1 root root 1367 3月  18 15:55 ca.pem
-rw------- 1 root root 1675 3月  18 15:21 node-3-key.pem
-rw-r--r-- 1 root root 1456 3月  18 15:21 node-3.pem
```

#### 2.4 ETCD

------

`cd /etc/etcd` 

```shell
root@node-8:/etc/etcd# ll
-rw-r--r--   1 root root  1367 3月  17 18:19 ca.pem
-rw-------   1 root root  1675 3月  17 18:19 kubernetes-key.pem
-rw-r--r--   1 root root  1639 3月  17 18:19 kubernetes.pem
```

### 3、程序目录

#### 3.1 Master

------

`cd /etc/systemd/system`

```shell
root@node-1:/etc/systemd/system# ll | grep kube
# 单群master
-rw-r--r--  1 root root 2051 3月  22 15:58 kube-apiserver.service
-rw-r--r--  1 root root  807 3月  17 22:41 kube-controller-manager.service
-rw-r--r--  1 root root  483 3月  21 10:28 kube-scheduler.service
```

#### 3.2 Master|Worker

------

`cd /etc/systemd/system`

```shell
root@node-2:/etc/systemd/system# ll | grep kube
# master和worker
-rw-r--r--  1 root root 1269 3月  18 10:48 containerd.service
-rw-r--r--  1 root root 2051 3月  22 16:02 kube-apiserver.service
-rw-r--r--  1 root root  807 3月  17 22:41 kube-controller-manager.service
-rw-r--r--  1 root root  585 3月  18 15:25 kubelet.service
-rw-r--r--  1 root root  265 3月  18 15:32 kube-proxy.service
-rw-r--r--  1 root root  483 3月  21 10:29 kube-scheduler.service
```

#### 3.3 Worker

------

`cd /etc/systemd/system`

```shell
root@node-3:/etc/systemd/system# ll | grep kube
root@node-3:/etc/systemd/system# ll | grep containerd.service 
# 单群worker
-rw-r--r--  1 root root 1269 3月  18 10:48 containerd.service
-rw-r--r--  1 root root  585 3月  18 15:25 kubelet.service
-rw-r--r--  1 root root  265 3月  18 15:32 kube-proxy.service
```

#### 3.4 ETCD

------

`cd /etc/systemd/system`

```shell
root@node-8:/etc/systemd/system# ll | grep etc
# 单群ETCD
-rw-r--r--  1 root root  981 3月  17 18:37 etcd.service
```



## 三、Kubernetes集群部署

### 1、环境准备

- 1.1 修改主机名（所有机器）

  ```shell
  vi /etc/hostname
  hostname #自定义
  ```

- 1.2 本地域名解析（所有机器）

  ```
  $ vi /etc/hosts
  # <ip> <hostname>
  如下格式：
  210.37.68.73 node-1
  210.37.68.74 node-2
  210.37.68.75 node-3
  210.37.68.76 node-4
  210.37.68.77 node-5
  210.37.68.78 node-6
  210.37.68.79 node-7
  210.37.68.80 node-8
  ```

- 1.3 关闭防火墙、swap分区、selinux、dnsmasq（否则可能导致容器无法解析域名）（所有机器）

  ```shell
  # 关闭防火墙
  $ systemctl stop firewalld && systemctl disable firewalld
  # 关闭swap
  $ swapoff -a && free –h
  # 关闭selinux
  $ setenforce 0
  # 关闭dnsmasq(否则可能导致容器无法解析域名)
  $ service dnsmasq stop && systemctl disable dnsmasq
  ```

- 1.4 配置k8s参数（所有机器）

  ```shell
  # 制作配置文件
  $ cat > /etc/sysctl.d/kubernetes.conf <<EOF
  net.bridge.bridge-nf-call-ip6tables = 1
  net.bridge.bridge-nf-call-iptables = 1
  net.ipv4.ip_nonlocal_bind = 1
  net.ipv4.ip_forward = 1
  vm.swappiness = 0
  vm.overcommit_memory = 1
  EOF
  # 生效文件
  $ sysctl -p /etc/sysctl.d/kubernetes.conf
  ```

- 1.5 配置免密登陆（one of master ）

  ```shell
  # 看看是否已经存在rsa公钥
  $ cat ~/.ssh/id_rsa.pub
  # 如果不存在就创建一个新的
  $ ssh-keygen -t rsa
  # 把id_rsa.pub文件内容copy到其他机器的授权文件中
  $ cat ~/.ssh/id_rsa.pub
  # 在其他节点执行下面命令（所有机器）
  $ echo "<file_content>" >> ~/.ssh/authorized_keys
  ```

- 1.6 k8s软件包下载（one of master）

  ```shell
  # 设定版本号
  $ export VERSION=v1.20.2
  # 下载master节点组件
  $ wget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kube-apiserver
  $ wget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kube-controller-manager
  $ wget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kube-scheduler
  $ wget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kubectl
  # 下载worker节点组件
  $ wget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kube-proxy
  $ wget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kubelet
  # 下载etcd组件
  $ wget https://github.com/etcd-io/etcd/releases/download/v3.4.10/etcd-v3.4.10-linux-amd64.tar.gz
  $ tar -xvf etcd-v3.4.10-linux-amd64.tar.gz
  $ mv etcd-v3.4.10-linux-amd64/etcd* .
  $ rm -fr etcd-v3.4.10-linux-amd64*
  # 统一修改文件权限为可执行
  $ chmod +x kube*
  ```

- 1.7 软件包分发（one of master）

  ```shell
  # 注意涉及到key-value(类似MASTERS=(node-1 node-2))
  # 设置的时候转成bash模式，否则变量输出会有问题。
  
  # 1、master相关组件分发到master节点
  $ MASTERS=(node-1 node-2)
  for instance in ${MASTERS[@]}; do
    scp kube-apiserver kube-controller-manager kube-scheduler kubectl root@${instance}:/usr/local/bin/
  done
  # 2、worker先关组件分发到worker节点
  $ WORKERS=(node-2 node-3 node-4 node-5 node-6 node-7 node-8)
  $ for instance in ${WORKERS[@]}; do
    scp kubelet kube-proxy root@${instance}:/usr/local/bin/
  done
  # 3、etcd组件分发到etcd节点
  $ ETCDS=(node-6 node-7 node-8)
  $ for instance in ${ETCDS[@]}; do
    scp etcd etcdctl root@${instance}:/usr/local/bin/
  done
  ```

### 2、签发证书

- **2.1 安装cfssl（one of master）**

  ```shell
  # 下载
  $ wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /usr/local/bin/cfssl
  $ wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /usr/local/bin/cfssljson
  # 修改为可执行权限
  $ chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson
  # 验证
  $ cfssl version
  ```

- **2.2 根证书（one of master）**

  ```shell
  $ cat > ca-config.json <<EOF
  {
    "signing": {
      "default": {
        "expiry": "876000h"
      },
      "profiles": {
        "kubernetes": {
          "usages": ["signing", "key encipherment", "server auth", "client auth"],
          "expiry": "876000h"
        }
      }
    }
  }
  EOF
  
  $ cat > ca-csr.json <<EOF
  {
    "CN": "Kubernetes",
    "key": {
      "algo": "rsa",
      "size": 2048
    },
    "names": [
      {
        "C": "US",
        "L": "Portland",
        "O": "Kubernetes",
        "OU": "CA",
        "ST": "Oregon"
      }
    ]
  }
  EOF
  ```

  `执行`

  ```shell
  # 生成证书和私钥
  $ cfssl gencert -initca ca-csr.json | cfssljson -bare ca
  # 生成完成后会有以下文件（我们最终想要的就是ca-key.pem和ca.pem，一个秘钥，一个证书）
  $ ls
  ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem
  ```

- **admin客户端证书（one of master）**

  ```shell
  $ cat > admin-csr.json <<EOF
  {
    "CN": "admin",
    "key": {
      "algo": "rsa",
      "size": 2048
    },
    "names": [
      {
        "C": "CN",
        "ST": "BeiJing",
        "L": "BeiJing",
        "O": "system:masters",
        "OU": "seven"
      }
    ]
  }
  EOF
  ```

  `执行`

  ```shell
  $ cfssl gencert \
    -ca=ca.pem \
    -ca-key=ca-key.pem \
    -config=ca-config.json \
    -profile=kubernetes \
    admin-csr.json | cfssljson -bare admin
  ```

- **kubelet客户端证书（one of master）**

  ```shell
  # 所有worker节点node列表
  $ WORKERS=(node-2 node-3)
  # 所有worker节点IP列表
  $ WORKER_IPS=(10.155.19.64 10.155.19.147)
  
  # 生成所有worker节点的证书配置
  $ for ((i=0;i<${#WORKERS[@]};i++)); do
  cat > ${WORKERS[$i]}-csr.json <<EOF
  {
    "CN": "system:node:${WORKERS[$i]}",
    "key": {
      "algo": "rsa",
      "size": 2048
    },
    "names": [
      {
        "C": "CN",
        "L": "Beijing",
        "O": "system:nodes",
        "OU": "seven",
        "ST": "Beijing"
      }
    ]
  }
  EOF
  cfssl gencert \
    -ca=ca.pem \
    -ca-key=ca-key.pem \
    -config=ca-config.json \
    -hostname=${WORKERS[$i]},${WORKER_IPS[$i]} \
    -profile=kubernetes \
    ${WORKERS[$i]}-csr.json | cfssljson -bare ${WORKERS[$i]}
  done
  ```

- **kube-controller-manager客户端证书**

  ```shell
  $ cat > kube-controller-manager-csr.json <<EOF
  {
      "CN": "system:kube-controller-manager",
      "key": {
          "algo": "rsa",
          "size": 2048
      },
      "names": [
        {
          "C": "CN",
          "ST": "BeiJing",
          "L": "BeiJing",
          "O": "system:kube-controller-manager",
          "OU": "seven"
        }
      ]
  }
  EOF
  ```

  `执行`

  ```shell
  $ cfssl gencert \
    -ca=ca.pem \
    -ca-key=ca-key.pem \
    -config=ca-config.json \
    -profile=kubernetes \
    kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager
  ```

- **kube-proxy客户端证书**

  ```shell
  $ cat > kube-proxy-csr.json <<EOF
  {
    "CN": "system:kube-proxy",
    "key": {
      "algo": "rsa",
      "size": 2048
    },
    "names": [
      {
        "C": "CN",
        "ST": "BeiJing",
        "L": "BeiJing",
        "O": "k8s",
        "OU": "seven"
      }
    ]
  }
  EOF
  ```

  `执行`

  ```shell
  $ cfssl gencert \
    -ca=ca.pem \
    -ca-key=ca-key.pem \
    -config=ca-config.json \
    -profile=kubernetes \
    kube-proxy-csr.json | cfssljson -bare kube-proxy
  ```

- **kube-scheduler客户端证书**

  ```shell
  $ cat > kube-scheduler-csr.json <<EOF
  {
      "CN": "system:kube-scheduler",
      "key": {
          "algo": "rsa",
          "size": 2048
      },
      "names": [
        {
          "C": "CN",
          "ST": "BeiJing",
          "L": "BeiJing",
          "O": "system:kube-scheduler",
          "OU": "seven"
        }
      ]
  }
  EOF
  ```

  `执行`

  ```shell
  $ cfssl gencert \
    -ca=ca.pem \
    -ca-key=ca-key.pem \
    -config=ca-config.json \
    -profile=kubernetes \
    kube-scheduler-csr.json | cfssljson -bare kube-scheduler
  ```

- **kube-apiserver服务端证书**

  ```shell
  $ cat > kubernetes-csr.json <<EOF
  {
    "CN": "kubernetes",
    "key": {
      "algo": "rsa",
      "size": 2048
    },
    "names": [
      {
        "C": "CN",
        "ST": "BeiJing",
        "L": "BeiJing",
        "O": "k8s",
        "OU": "seven"
      }
    ]
  }
  EOF
  ```

  `执行`

  ```shell
  # apiserver的service ip地址（一般是svc网段的第一个ip,如果不理解就直接用这个IP就好了）
  $ KUBERNETES_SVC_IP=10.233.0.1
  # 所有的master和etcd的ip
  # 注意这里一定要将etcd的ip列表加进来否则etcd集群无法验证通过，导致etcd集群无法起来
  $ MASTER_IPS=10.155.19.223,10.155.19.64,10.155.19.147
  # 生成证书
  $ cfssl gencert \
    -ca=ca.pem \
    -ca-key=ca-key.pem \
    -config=ca-config.json \
    -hostname=${KUBERNETES_SVC_IP},${MASTER_IPS},127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local \
    -profile=kubernetes \
    kubernetes-csr.json | cfssljson -bare kubernetes
  ```




## 四、扩容worker

**以下操作都在新加的worker服务器上执行，如果不是在worker上的操作会有特殊描述，阅读时候请注意。**

### 1、修改主机名

```shell
# 查看主机名
$ hostname
# 修改主机名
$ hostnamectl set-hostname <your_hostname>
# 配置host，使主节点之间可以通过hostname互相访问
$ vi /etc/hosts
# <ip> <hostname>
```

### 2、修改hosts

```shell
#Kubernetes
$ vi /etc/hosts
192.168.10.121 node-1
192.168.10.122 node-2
192.168.10.123 node-3
192.168.10.124 node-4
192.168.10.4 node-5
```

### 3、关闭防火墙、selinux、swap，重置iptables，配置文件

```shell
# 关闭防火墙
$ systemctl stop firewalld && systemctl disable firewalld
# 关闭swap
$ swapoff -a && free –h
# 关闭selinux
$ setenforce 0
# 关闭dnsmasq(否则可能导致容器无法解析域名)
$ service dnsmasq stop && systemctl disable dnsmasq
```

`配置文件`

```shell
# 制作配置文件
$ cat > /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_nonlocal_bind = 1
net.ipv4.ip_forward = 1
vm.swappiness = 0
vm.overcommit_memory = 1
EOF
# 生效文件
$ sysctl -p /etc/sysctl.d/kubernetes.conf
```

### 4、免密登陆配置

```bash
# 看看是否已经存在rsa公钥
$ cat ~/.ssh/id_rsa.pub
# 如果不存在就创建一个新的
$ ssh-keygen -t rsa
# 把id_rsa.pub文件内容copy到其他机器的授权文件中
$ cat ~/.ssh/id_rsa.pub
# 在其他节点执行下面命令（包括worker节点）
$ echo "<file_content>" >> ~/.ssh/authorized_keys
例如：在待新加入的节点node-new服务器上执行以下操作
echo "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGVY93IOuYzT7C1Z6ZsyqNjDYMzF9QsiRE2rYVjtN+yQ18zVEmwPGVvHrJgqx7TDd2ir2cfMi9whpADA65L/LHDub2PK1OSB5OMdS2gaMFoSkoCtzlz+nkkEH0YGIRBbJUJ944Ha8MWSwWEHd0K/7F+F1Y2DpPMfRT4Ohaond2oKYnDA0r8LnOOJSMdMprGBnVtRdSR+8fxgJadGhbJReLjyJRdrMzW1cvJUXfp2DeR68jS7fxOd2vEV8+8S679aJvIwc+3X51WNYaKHx0I4fRMMvFusIFPZxD6G9h6Lm+mzVpFIgFfopfcyQ3QRO4sqSKexbRoCHm8YXNlq3RuKbB root@fabric1" >> ~/.ssh/authorized_keys
```

`杭州环境有效pk`

```txt
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGVY93IOuYzT7C1Z6ZsyqNjDYMzF9QsiRE2rYVjtN+yQ18zVEmwPGVvHrJgqx7TDd2ir2cfMi9whpADA65L/LHDub2PK1OSB5OMdS2gaMFoSkoCtzlz+nkkEH0YGIRBbJUJ944Ha8MWSwWEHd0K/7F+F1Y2DpPMfRT4Ohaond2oKYnDA0r8LnOOJSMdMprGBnVtRdSR+8fxgJadGhbJReLjyJRdrMzW1cvJUXfp2DeR68jS7fxOd2vEV8+8S679aJvIwc+3X51WNYaKHx0I4fRMMvFusIFPZxD6G9h6Lm+mzVpFIgFfopfcyQ3QRO4sqSKexbRoCHm8YXNlq3RuKbB root@fabric1
```

### 5、把worker相关组件分发到worker服务器

**以下操作在Master（192.168.10.121）上操作**

- 5.1 <u>进入目录</u>

```bash
cd /usr/local/kubernetes/kubernetes-v1.20.2
```

![image-20220606153424115](C:\Users\dev\AppData\Roaming\Typora\typora-user-images\image-20220606153424115.png)

- <u>5.2 将kubelet和kube-proxy拷贝到新增的worker节点的目录中，如果服务器上没有可以自行下载。</u>

`不存在软件包`

```bash
# 设定版本号
$ export VERSION=v1.20.2
# 下载worker节点组件
$ wget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kube-proxy
$ wget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kubelet
# 统一修改文件权限为可执行
$ chmod +x kube*
```

`存在软件包`

```bash
存在直接copy：
# 把worker先关组件分发到worker节点
$ WORKERS=(node-new)
for instance in ${WORKERS[@]}; do
  scp kubelet kube-proxy root@${instance}:/usr/local/bin/
done
```

### 6、安装cfssl

```shell
# 下载
$ wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /usr/local/bin/cfssl
$ wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /usr/local/bin/cfssljson
# 修改为可执行权限
$ chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson
# 验证
$ cfssl version
```

### 7、签发worker相关证书

**以下操作在Master（192.168.10.121）上操作**

`7.1 cd /root/pki`

![image-20220606160421109](C:\Users\dev\AppData\Roaming\Typora\typora-user-images\image-20220606160421109.png)

`7.2 生成kubelet客户端证书和私钥`

```shell
# 设置你的worker节点列表
$ WORKERS=(node-new node-new-1) #多个空格隔开
$ WORKER_IPS=(192.168.10.1 192.168.10.2) #多个空格隔开
# 生成所有worker节点的证书配置
$ for ((i=0;i<${#WORKERS[@]};i++)); do
cat > ${WORKERS[$i]}-csr.json <<EOF
{
  "CN": "system:node:${WORKERS[$i]}",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "Beijing",
      "O": "system:nodes",
      "OU": "seven",
      "ST": "Beijing"
    }
  ]
}
EOF
cfssl gencert \
  -ca=ca.pem \
  -ca-key=ca-key.pem \
  -config=ca-config.json \
  -hostname=${WORKERS[$i]},${WORKER_IPS[$i]} \
  -profile=kubernetes \
  ${WORKERS[$i]}-csr.json | cfssljson -bare ${WORKERS[$i]}
done
```

生成新的worker所需证书

- node-5.csr
- node-5-csr.json
- node-5-key.pem
- node-5.kubeconfig
- node-5.pem

`7.3 分发worker证书`

```shell
# 设置你的worker节点列表
$ WORKERS=(node-x node-x-1)
for instance in ${WORKERS[@]}; do
  scp ca.pem ca-key.pem ${instance}-key.pem ${instance}.pem root@${instance}:~/
done
```

### 8、签发worker证书认证配置

**以下操作在Master（192.168.10.121）上操作**

```shell
# 指定你的worker列表（hostname），空格分隔
$ WORKERS="node-new node-new-1"
$ for instance in ${WORKERS}; do
  kubectl config set-cluster kubernetes \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=${instance}.kubeconfig

  kubectl config set-credentials system:node:${instance} \
    --client-certificate=${instance}.pem \
    --client-key=${instance}-key.pem \
    --embed-certs=true \
    --kubeconfig=${instance}.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes \
    --user=system:node:${instance} \
    --kubeconfig=${instance}.kubeconfig

  kubectl config use-context default --kubeconfig=${instance}.kubeconfig
done
```

`分发证书`

```shell
$ WORKERS="node-new node-new-1"
$ for instance in ${WORKERS}; do
    scp ${instance}.kubeconfig kube-proxy.kubeconfig ${instance}:~/
done
```

### 9、安装Containerd

**以下操作在Worker（192.168.10.XX）上操作**

`软件包下载`

```shell
# 设定containerd的版本号
$ VERSION=1.4.3
# 下载压缩包
$ wget https://github.com/containerd/containerd/releases/download/v${VERSION}/cri-containerd-cni-${VERSION}-linux-amd64.tar.gz
```

`解压安装包`

```shell
# 解压缩
$ tar -xvf cri-containerd-cni-${VERSION}-linux-amd64.tar.gz
# 复制需要的文件
$ cp etc/crictl.yaml /etc/
$ cp etc/systemd/system/containerd.service /etc/systemd/system/
$ cp -r usr /
```

![image-20220606180426459](C:\Users\dev\AppData\Roaming\Typora\typora-user-images\image-20220606180426459.png)

`配置containerd文件`

```shell
$ mkdir -p /etc/containerd
# 默认配置生成配置文件
$ containerd config default > /etc/containerd/config.toml
# 定制化配置（可选）
$ vi /etc/containerd/config.toml
```

**config.toml配置模板**

![image-20220606181151463](C:\Users\dev\AppData\Roaming\Typora\typora-user-images\image-20220606181151463.png)

```yaml
version = 2
root = "/var/lib/containerd"
state = "/run/containerd"
plugin_dir = ""
disabled_plugins = []
required_plugins = []
oom_score = 0

[grpc]
  address = "/run/containerd/containerd.sock"
  tcp_address = ""
  tcp_tls_cert = ""
  tcp_tls_key = ""
  uid = 0
  gid = 0
  max_recv_message_size = 16777216
  max_send_message_size = 16777216

[ttrpc]
  address = ""
  uid = 0
  gid = 0

[debug]
  address = ""
  uid = 0
  gid = 0
  level = ""

[metrics]
  address = ""
  grpc_histogram = false

[cgroup]
  path = ""

[timeouts]
  "io.containerd.timeout.shim.cleanup" = "5s"
  "io.containerd.timeout.shim.load" = "5s"
  "io.containerd.timeout.shim.shutdown" = "3s"
  "io.containerd.timeout.task.state" = "2s"

[plugins]
  [plugins."io.containerd.gc.v1.scheduler"]
    pause_threshold = 0.02
    deletion_threshold = 0
    mutation_threshold = 100
    schedule_delay = "0s"
    startup_delay = "100ms"
  [plugins."io.containerd.grpc.v1.cri"]
    disable_tcp_service = true
    stream_server_address = "127.0.0.1"
    stream_server_port = "0"
    stream_idle_timeout = "4h0m0s"
    enable_selinux = false
    selinux_category_range = 1024
    sandbox_image = "k8s.gcr.io/pause:3.2"
    stats_collect_period = 10
    systemd_cgroup = false
    enable_tls_streaming = false
    max_container_log_line_size = 16384
    disable_cgroup = false
    disable_apparmor = false
    restrict_oom_score_adj = false
    max_concurrent_downloads = 3
    disable_proc_mount = false
    unset_seccomp_profile = ""
    tolerate_missing_hugetlb_controller = true
    disable_hugetlb_controller = true
    ignore_image_defined_volumes = false
    [plugins."io.containerd.grpc.v1.cri".containerd]
      snapshotter = "overlayfs"
      default_runtime_name = "runc"
      no_pivot = false
      disable_snapshot_annotations = true
      discard_unpacked_layers = false
      [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime]
        runtime_type = ""
        runtime_engine = ""
        runtime_root = ""
        privileged_without_host_devices = false
        base_runtime_spec = ""
      [plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime]
        runtime_type = ""
        runtime_engine = ""
        runtime_root = ""
        privileged_without_host_devices = false
        base_runtime_spec = ""
      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          runtime_type = "io.containerd.runc.v2"
          runtime_engine = ""
          runtime_root = ""
          privileged_without_host_devices = false
          base_runtime_spec = ""
          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
    [plugins."io.containerd.grpc.v1.cri".cni]
      bin_dir = "/opt/cni/bin"
      conf_dir = "/etc/cni/net.d"
      max_conf_num = 1
      conf_template = ""
    [plugins."io.containerd.grpc.v1.cri".registry]
      [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
        [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
          endpoint = ["https://registry-1.docker.io"]
      [plugins."io.containerd.grpc.v1.cri".registry.configs]
        [plugins."io.containerd.grpc.v1.cri".registry.configs."nvxhub.nvxclouds.net:9443".tls]
          insecure_skip_verify = true
        [plugins."io.containerd.grpc.v1.cri".registry.configs."nvxhub.nvxclouds.net:9443".auth]
          username = "huzhengxing"
          password = "NVXClouds_168"
      




    [plugins."io.containerd.grpc.v1.cri".image_decryption]
      key_model = ""
    [plugins."io.containerd.grpc.v1.cri".x509_key_pair_streaming]
      tls_cert_file = ""
      tls_key_file = ""
  [plugins."io.containerd.internal.v1.opt"]
    path = "/opt/containerd"
  [plugins."io.containerd.internal.v1.restart"]
    interval = "10s"
  [plugins."io.containerd.metadata.v1.bolt"]
    content_sharing_policy = "shared"
  [plugins."io.containerd.monitor.v1.cgroups"]
    no_prometheus = false
  [plugins."io.containerd.runtime.v1.linux"]
    shim = "containerd-shim"
    runtime = "runc"
    runtime_root = ""
    no_shim = false
    shim_debug = false
  [plugins."io.containerd.runtime.v2.task"]
    platforms = ["linux/amd64"]
  [plugins."io.containerd.service.v1.diff-service"]
    default = ["walking"]
  [plugins."io.containerd.snapshotter.v1.devmapper"]
    root_path = ""
    pool_name = ""
    base_image_size = ""
    async_remove = false
```

`启动containerd`

```shell
$ systemctl enable containerd
$ systemctl restart containerd
$ systemctl status containerd
```

### 10、配置kubelet

**以下操作在Worker（192.168.10.XX）上操作**

```shell
$ mkdir -p /etc/kubernetes/ssl/
$ mv ${HOSTNAME}-key.pem ${HOSTNAME}.pem ca.pem ca-key.pem /etc/kubernetes/ssl/
$ mkdir -p /etc/kubernetes/kubeconfig/
$ mv ${HOSTNAME}.kubeconfig /etc/kubernetes/kubeconfig
$ IP=10.155.19.64  #新加入的worker ip
# 写入kubelet配置文件
$ cat <<EOF > /etc/kubernetes/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/etc/kubernetes/ssl/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "169.254.25.10"
podCIDR: "10.200.0.0/16"
address: ${IP}
readOnlyPort: 0
staticPodPath: /etc/kubernetes/manifests
healthzPort: 10248
healthzBindAddress: 127.0.0.1
kubeletCgroups: /systemd/system.slice
resolvConf: "/etc/resolv.conf"
runtimeRequestTimeout: "15m"
kubeReserved:
  cpu: 200m
  memory: 512M
tlsCertFile: "/etc/kubernetes/ssl/${HOSTNAME}.pem"
tlsPrivateKeyFile: "/etc/kubernetes/ssl/${HOSTNAME}-key.pem"
EOF
```

`配置kubelet服务sevice`

```shell
$ cat <<EOF > /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --config=/etc/kubernetes/kubelet-config.yaml \\
  --container-runtime=remote \\
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/etc/kubernetes/kubeconfig \\
  --network-plugin=cni \\
  --node-ip=${IP} \\
  --register-node=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

### 11、配置nginx代理api-server

**以下操作在Worker（192.168.10.XX）上操作**

**注意：nginx-proxy 只需要在没有 apiserver 的节点部署**

`nginx配置文件`

```shell
$ mkdir -p /etc/nginx
# master ip列表
$ MASTER_IPS=(192.168.10.121 192.168.10.122)
# 执行前请先copy一份，并修改好upstream的 'server' 部分配置
$ cat <<EOF > /etc/nginx/nginx.conf
error_log stderr notice;

worker_processes 2;
worker_rlimit_nofile 130048;
worker_shutdown_timeout 10s;

events {
  multi_accept on;
  use epoll;
  worker_connections 16384;
}

stream {
  upstream kube_apiserver {
    least_conn;
    server ${MASTER_IPS[0]}:6443;
    server ${MASTER_IPS[1]}:6443;
  }

  server {
    listen        127.0.0.1:6443;
    proxy_pass    kube_apiserver;
    proxy_timeout 10m;
    proxy_connect_timeout 1s;
  }
}

http {
  aio threads;
  aio_write on;
  tcp_nopush on;
  tcp_nodelay on;

  keepalive_timeout 5m;
  keepalive_requests 100;
  reset_timedout_connection on;
  server_tokens off;
  autoindex off;

  server {
    listen 8081;
    location /healthz {
      access_log off;
      return 200;
    }
    location /stub_status {
      stub_status on;
      access_log off;
    }
  }
}
EOF
```

`nginx manifest`

```shell
$ mkdir -p /etc/kubernetes/manifests/
$ cat <<EOF > /etc/kubernetes/manifests/nginx-proxy.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-proxy
  namespace: kube-system
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
    k8s-app: kube-nginx
spec:
  hostNetwork: true
  dnsPolicy: ClusterFirstWithHostNet
  nodeSelector:
    kubernetes.io/os: linux
  priorityClassName: system-node-critical
  containers:
  - name: nginx-proxy
    image: docker.io/library/nginx:1.19
    imagePullPolicy: IfNotPresent
    resources:
      requests:
        cpu: 25m
        memory: 32M
    securityContext:
      privileged: true
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8081
    readinessProbe:
      httpGet:
        path: /healthz
        port: 8081
    volumeMounts:
    - mountPath: /etc/nginx
      name: etc-nginx
      readOnly: true
  volumes:
  - name: etc-nginx
    hostPath:
      path: /etc/nginx
EOF
```

### 12、配置kube-proxy

**以下操作在Worker（192.168.10.XX）上操作**

`配置文件`

```shell
$ cp kube-proxy.kubeconfig /etc/kubernetes/
# 创建 kube-proxy-config.yaml
$ cat <<EOF > /etc/kubernetes/kube-proxy-config.yaml
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
bindAddress: 0.0.0.0
clientConnection:
  kubeconfig: "/etc/kubernetes/kube-proxy.kubeconfig"
clusterCIDR: "10.200.0.0/16"
mode: ipvs
EOF
```

`kube-proxy服务文件service`

```shell
$ cat <<EOF > /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/etc/kubernetes/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

`以上过程最后在worker目录：/etc/kubernetes展示`

![image-20220606191351044](C:\Users\dev\AppData\Roaming\Typora\typora-user-images\image-20220606191351044.png)

```shell
-rw-r--r--   1 root root 6365 May 31 18:00 kubeconfig
-rw-r--r--   1 root root  689 May 31 18:52 kubelet-config.yaml
-rw-r--r--   1 root root  207 May 31 18:58 kube-proxy-config.yaml
-rw-r--r--   1 root root 6299 Dec 17 12:13 kube-proxy.kubeconfig
drwxr-xr-x   2 root root 4096 May 31 18:56 manifests/
drwxr-xr-x   3 root root 4096 May 31 17:52 ssl/
```

`manifests目录`

![image-20220606191554885](C:\Users\dev\AppData\Roaming\Typora\typora-user-images\image-20220606191554885.png)

`ssl目录`

```shell
-rw-r--r-- 1 root root 1679 Dec 17 11:56 ca-key.pem
-rw-r--r-- 1 root root 1367 Dec 17 11:56 ca.pem
-rw-r--r-- 1 root root 1675 May 31 17:49 node-5-key.pem
-rw-r--r-- 1 root root 1456 May 31 17:49 node-5.pem
```

### 13、启动kubelet和kube-proxy

`拉取所有worker必要的镜像`

![image-20220610204738879](C:\Users\dev\AppData\Roaming\Typora\typora-user-images\image-20220610204738879.png)

`执行以下拉取镜像命令`

```shell
crictl pull registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/pause:3.2 && \
ctr -n k8s.io i tag  registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/pause:3.2 k8s.gcr.io/pause:3.2 && \
crictl pull docker.io/calico/cni:v3.21.2 && \
crictl pull docker.io/calico/kube-controllers:v3.21.2 && \
crictl pull docker.io/library/nginx:1.19 && \
crictl pull docker.io/calico/node:v3.21.2  && \
crictl pull docker.io/calico/pod2daemon-flexvol:v3.21.2  && \
crictl pull docker.io/coredns/coredns:1.6.7 && \
crictl pull registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/dns_k8s-dns-node-cache:1.16.0
```

`执行启动kubelet、kube-peoxy`

```shell
$ systemctl daemon-reload && \
 systemctl enable kubelet kube-proxy && \
 systemctl restart kubelet kube-proxy
$ journalctl -f -u kubelet
$ journalctl -f -u kube-proxy
```

`如下表示启动成功`

![image-20220606190516590](C:\Users\dev\AppData\Roaming\Typora\typora-user-images\image-20220606190516590.png)





## 五、杭州环境（IDC）

### 1、证书签发目录

```shell
/root/pki
```

### 2、服务器资源

| 序号 | 系统   | 主机名 | 状态 | IP             | 内存/G | 磁盘/G | 角色          | 工作目录         | 备注 |
| ---- | ------ | ------ | ---- | -------------- | ------ | ------ | ------------- | ---------------- | ---- |
| 1    | Ubuntu | node-1 | 可用 | 192.168.10.121 | 15     | 980    | Master        | /root/kubernetes | 普通 |
| 2    | Ubuntu | node-2 | 可用 | 192.168.10.122 | 15     | 980    | Master/Worker | /root/kubernetes | 普通 |
| 3    | Ubuntu | node-3 | 可用 | 192.168.10.123 | 15     | 980    | Worker        |                  | 普通 |
| 4    | Linux  | node-5 | 可用 | 192.168.10.4   | 15     | 980    | Worker        |                  | TEE  |
| 5    | Ubuntu | node-4 | 可用 | 192.168.10.124 | 15     | 980    | Worker        |                  | 普通 |

![image-20220606145501317](C:\Users\dev\AppData\Roaming\Typora\typora-user-images\image-20220606145501317.png)

### 3、Dashboard

- https://192.168.10.122:30141/

- token

  ```tex
  eyJhbGciOiJSUzI1NiIsImtpZCI6IkxrSlhneVN5ampJSUhTVFFnUlZad1U5QV9LTWlfZTVteURuSlludm11T0UifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tNzR2anciLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZWIxMGI0M2ItOTM4My00MzE3LWFkY2MtNTYwNjc3OWI1ZDU1Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRhc2hib2FyZC1hZG1pbiJ9.Ov6t6B-UHIcDPt-Mcmsh8VlJ_vSPQ5VmLCFCdDxh0yW1SuQVrA0jHA_NEOmc74kjF9sJXUFFOZveOYRwRRsH9MLR4VZ5i_uQ_AlcioJiL8_eu8kD-zIyM33vDioQXAF3NsdjFdaXENS1-xAf3jxcWX_BcbWWR8rlMos8DP8h5peQ9NrVKtewrwt4EzofqKtVhbu84nqRCMSMA7YU9vJ3l7WlWZWPF7X6RXqFLKVM8gm2jkVCdMUm1MLdTFaVIBtY_Wr62MEZnKVyVtbCWbQZuuTn9tg5cO6XBKv-a3tNaKD6dq9AyOobbjplJcN8Wxn0VhNrl1c_YGI3Up9Ine362Q
  ```

  

### 4、Whoam测试

- http://nv.study.com:8080/



### 5、Nginx Ingress

| 序号 | Pod名称                                   | Worker                | 部署文件                                                     | 备注 |
| ---- | ----------------------------------------- | --------------------- | ------------------------------------------------------------ | ---- |
| 1    | nginx-ingress-controller-665997d7f8-zfkzq | node-3/192.168.10.123 | /root/kubernetes/deploy_work/ingress-nginx/gp-mandatory.yaml |      |

`gp-mandatory.yaml`

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---

kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: tcp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: udp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses/status
    verbs:
      - update

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: nginx-ingress-role
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      # Defaults to "<election-id>-<ingress-class>"
      # Here: "<ingress-controller-leader>-<nginx>"
      # This has to be adapted if you change either parameter
      # when launching the nginx-ingress-controller.
      - "ingress-controller-leader-nginx"
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
      annotations:
        prometheus.io/port: "10254"
        prometheus.io/scrape: "true"
    spec:
      # wait up to five minutes for the drain of connections
      terminationGracePeriodSeconds: 300
      serviceAccountName: nginx-ingress-serviceaccount
      hostNetwork: true
      nodeSelector:
        app: ingress
        kubernetes.io/os: linux
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.26.1
          args:
            - /nginx-ingress-controller
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io
            # 增加以下两个参数参数（一个是http端口，一个是https端口）
            - --http-port=8080
            - --https-port=8443
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              drop:
                - ALL
              add:
                - NET_BIND_SERVICE
            # www-data -> 33
            runAsUser: 33
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
            - name: https
              containerPort: 443
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          lifecycle:
            preStop:
              exec:
                command:
                  - /wait-shutdown

---
```











